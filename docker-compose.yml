services:
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    env_file:
      - .env
    environment:
      POSTGRES_DB: callscribe
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 10

  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    env_file:
      - .env
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: bYKVzimvT8eN9QpL0dVhVA
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list"]
      interval: 10s
      timeout: 10s
      retries: 10

  api:
    build: .
    container_name: callscribe-api
    env_file:
      - .env
    environment:
      APP_NAME: "CallScribe API"
      APP_VERSION: "1.0.0"
      DEBUG: "False"
      DATABASE_URL: "postgresql://postgres:postgres@postgres:5432/callscribe"
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      KAFKA_AUDIO_TOPIC: "audio-topic"
      KAFKA_TRANSCRIPTION_TOPIC: "transcription-topic"
      LLM_API_BASE_URL: "http://ollama:11434/v1"
      LLM_API_KEY: "ollama"
      LLM_MODEL: "gemma:2b"
      MAX_FILE_SIZE_MB: 500
      UPLOAD_DIR: "/tmp/uploads"
      METRICS_ENABLED: "True"
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    volumes:
      - uploads:/tmp/uploads
    ports:
      - "8000:8000"

  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: prometheus
    env_file:
      - .env
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    ports:
      - "9090:9090"
    depends_on:
      api:
        condition: service_started

  grafana:
    image: grafana/grafana-oss:10.4.3
    container_name: grafana
    env_file:
      - .env
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    depends_on:
      prometheus:
        condition: service_started

  ui:
    build:
      context: .
      dockerfile: ui/Dockerfile
    container_name: callscribe-ui
    env_file:
      - .env
    environment:
      API_BASE_URL: http://api:8000
    ports:
      - "8501:8501"
    depends_on:
      api:
        condition: service_started

  asr-worker:
    build:
      context: .
      dockerfile: workers/asr/Dockerfile
      args:
        HF_TOKEN: ${HF_TOKEN}
        ASR_MODEL_REVISION: ${ASR_MODEL_REVISION:-e2e_rnnt}
    env_file:
      - .env
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_AUDIO_TOPIC: ${KAFKA_AUDIO_TOPIC}
      KAFKA_TRANSCRIPTION_TOPIC: ${KAFKA_TRANSCRIPTION_TOPIC}
      HF_TOKEN: ${HF_TOKEN}
      ASR_MODEL_ID: ${ASR_MODEL_ID:-ai-sage/GigaAM-v3}
      ASR_MODEL_REVISION: ${ASR_MODEL_REVISION:-e2e_rnnt}
    volumes:
      - uploads:/tmp/uploads
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy

  llm-worker:
    build:
      context: .
      dockerfile: workers/llm/Dockerfile
      args:
        HF_TOKEN: ${HF_TOKEN}
        LLM_MODEL_ID: ${LLM_MODEL_ID:-google/gemma-2b}
        LLM_MODEL_REVISION: ${LLM_MODEL_REVISION:-main}
    env_file:
      - .env
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_TRANSCRIPTION_TOPIC: ${KAFKA_TRANSCRIPTION_TOPIC}
      HF_TOKEN: ${HF_TOKEN}
      LLM_API_BASE_URL: ${LLM_API_BASE_URL:-http://ollama:11434}
      LLM_MODEL: ${LLM_MODEL:-gemma2:2b}
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started

  ollama:
    image: ollama/ollama:0.3.12
    container_name: ollama
    environment:
      OLLAMA_HOST: 0.0.0.0
      LLM_MODEL: ${LLM_MODEL:-gemma2:2b}
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

volumes:
  pgdata:
  uploads:
  ollama:

